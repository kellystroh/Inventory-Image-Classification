{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, regularizers, initializers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.io import imshow\n",
    "from skimage.transform import rotate\n",
    "from skimage.filters.edges import convolve\n",
    "\n",
    "from skimage.io import imshow, imread\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import dill as pickle\n",
    "\n",
    "np.random.seed(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/test_labels.csv')\n",
    "df_train = pd.read_csv('data/train_labels.csv')\n",
    "\n",
    "bw_loaded = np.load('data/bw_images.npz')\n",
    "X_train= bw_loaded['a']\n",
    "X_test = bw_loaded['b']\n",
    "\n",
    "X_test = X_test.reshape(-1,80,60,1)\n",
    "X_train = X_train.reshape(-1,80,60,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model_functions import pick_ylabels, multi_index_counts, test_counts_by_cat, train_counts_by_cat, category_codes, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_ylabels(column):\n",
    "    y_train = df_train[column].copy().astype('category').cat.codes\n",
    "    y_test = df_test[column].copy().astype('category').cat.codes\n",
    "    return (y_train.values, y_test.values)\n",
    "def multi_index_counts(col, col2):\n",
    "    counts = df_test.groupby([col, col2]).count().id\n",
    "    return counts\n",
    "\n",
    "def category_codes(column):\n",
    "    _, ytest = pick_ylabels(column)\n",
    "    cat_codes = {}\n",
    "    cat_code_list = []\n",
    "    for i in range(len(df_test[column].value_counts().index)):\n",
    "        s = i\n",
    "        t = df_test[column].value_counts().index[i]\n",
    "        cat_codes[s] = t\n",
    "#         cat_code_list.append([s, t])\n",
    "\n",
    "#     for key in sorted(cat_codes):\n",
    "#         print(\"%s: %s\" % (key, cat_codes[key]))\n",
    "    return cat_codes\n",
    "\n",
    "def test_counts_by_cat(column):\n",
    "    _, ytest = pick_ylabels(column)\n",
    "    test_counts_dict = {}\n",
    "    test_counts = []\n",
    "    for i in range(len(df_test[column].value_counts().index)):\n",
    "        s = df_test[column].value_counts().values[i]\n",
    "        t = df_test[column].value_counts().index[i]\n",
    "        test_counts_dict[t] = s\n",
    "        test_counts.append([t, s])\n",
    "#     for i in sorted(test_counts):\n",
    "#         print(\"%s: %s\" % (test_counts[0], test_counts[1]))\n",
    "    return test_counts\n",
    "\n",
    "def train_counts_by_cat(column):\n",
    "    y_train = pick_ylabels(column)\n",
    "    train_counts_dict = {}\n",
    "    train_counts = []\n",
    "    for i in range(len(df_test[column].value_counts().index)):\n",
    "        s = df_train[column].value_counts().values[i]\n",
    "        t = df_train[column].value_counts().index[i]\n",
    "        train_counts_dict[t] = s\n",
    "        train_counts.append([t, s])\n",
    "#     for i in sorted(test_counts):\n",
    "#         print(\"%s: %s\" % (test_counts[0], test_counts[1]))\n",
    "    return test_counts\n",
    "\n",
    "def class_weights(column):\n",
    "    train_counts = train_counts_by_cat(column)\n",
    "    counts_list = []\n",
    "    ratio_list = []\n",
    "    ratio_dict = {}\n",
    "    for x in test_counts:\n",
    "        counts_list.append(x[1])\n",
    "    #print(counts_list)\n",
    "    z = sum(counts_list)\n",
    "    for x in counts_list:\n",
    "        if np.round(x/z, 3) ==0:\n",
    "            ratio_list.append(.001)\n",
    "        else: \n",
    "            ratio_list.append(np.round(x/z, 2))\n",
    "    for k, v in enumerate(ratio_list):\n",
    "        ratio_dict[k] = v\n",
    "    return ratio_dict\n",
    "\n",
    "def model_results(model, column):\n",
    "    yhat = model.predict(X_test)\n",
    "    _ , ytest = pick_ylabels(column)\n",
    "    accuracy = accuracy_score(ytest, yhat)\n",
    "    print( 'Accuracy Score: ', accuracy )\n",
    "    recall = recall_score(ytest, yhat, average='weighted')\n",
    "    print( 'Recall Score: ', recall)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    wrong_id_list = []\n",
    "    pred_cat_list = []\n",
    "    real_cat_list = []\n",
    "    for row_idx in range(len(ytest)):\n",
    "        if ytest[row_idx]!=yhat[row_idx]:\n",
    "            wrong_id_list.append(row_idx)\n",
    "            pred_cat_list.append(yhat[row_idx])\n",
    "            real_cat_list.append(ytest[row_idx])\n",
    "\n",
    "    arr = np.array([ real_cat_list, pred_cat_list])\n",
    "    arr = arr.transpose()\n",
    "    wrong_df = pd.DataFrame( arr, index= wrong_id_list, columns = ['actual', 'predicted'] )\n",
    "    return (accuracy, recall, wrong_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train0, y_test0 = pick_ylabels('masterCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train0)\n",
    "y_test = to_categorical(y_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25439, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn1 = models.Sequential()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 17:06:49.828440 140568897226560 deprecation.py:506] From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "cnn1.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(80,60,1)))\n",
    "#MaxPool reduces dimensionality of each feature\n",
    "cnn1.add(layers.MaxPooling2D((2, 2)))\n",
    "#Dropout to reduce overfitting\n",
    "cnn1.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 78, 58, 32)        320       \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten 3D outputs to 1D\n",
    "cnn1.add(Flatten())\n",
    "#there are 4 main categories, and softmax will calculate probabilities of each class\n",
    "cnn1.add(Dense(4, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25439 samples, validate on 6440 samples\n",
      "Epoch 1/10\n",
      "25439/25439 [==============================] - 17s 659us/sample - loss: 67.5887 - acc: 0.8980 - val_loss: 0.6874 - val_acc: 0.9547\n",
      "Epoch 2/10\n",
      "25439/25439 [==============================] - 14s 539us/sample - loss: 0.4225 - acc: 0.9689 - val_loss: 0.6469 - val_acc: 0.9648\n",
      "Epoch 3/10\n",
      "25439/25439 [==============================] - 14s 544us/sample - loss: 0.4069 - acc: 0.9671 - val_loss: 0.6649 - val_acc: 0.9512\n",
      "Epoch 4/10\n",
      "25439/25439 [==============================] - 13s 526us/sample - loss: 0.2014 - acc: 0.9790 - val_loss: 0.6316 - val_acc: 0.9590\n",
      "Epoch 5/10\n",
      "25439/25439 [==============================] - 14s 542us/sample - loss: 0.1618 - acc: 0.9815 - val_loss: 0.6002 - val_acc: 0.9609\n",
      "Epoch 6/10\n",
      "25439/25439 [==============================] - 14s 539us/sample - loss: 0.1191 - acc: 0.9841 - val_loss: 0.5987 - val_acc: 0.9590\n",
      "Epoch 7/10\n",
      "25439/25439 [==============================] - 14s 532us/sample - loss: 0.1401 - acc: 0.9829 - val_loss: 0.6175 - val_acc: 0.9554\n",
      "Epoch 8/10\n",
      "25439/25439 [==============================] - 13s 517us/sample - loss: 0.2325 - acc: 0.9687 - val_loss: 0.4644 - val_acc: 0.9455\n",
      "Epoch 9/10\n",
      "25439/25439 [==============================] - 13s 525us/sample - loss: 0.1565 - acc: 0.9728 - val_loss: 0.4624 - val_acc: 0.9519\n",
      "Epoch 10/10\n",
      "25439/25439 [==============================] - 13s 518us/sample - loss: 0.1009 - acc: 0.9784 - val_loss: 0.4937 - val_acc: 0.9399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd87c421cf8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn1.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the fitted model\n",
    "model_json = cnn1.to_json()\n",
    "with open(\"cnn1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions for the test data\n",
    "predicted_classes = cnn1.predict_classes(X_test)\n",
    "\n",
    "# get the indices to be plotted\n",
    "correct = np.nonzero(predicted_classes==y_test0)[0]\n",
    "incorrect = np.nonzero(predicted_classes!=y_test0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.88      0.92      0.90      1626\n",
      "     Class 1       0.97      0.95      0.96      2603\n",
      "     Class 2       0.98      0.96      0.97      1826\n",
      "     Class 3       0.80      0.84      0.82       385\n",
      "\n",
      "    accuracy                           0.94      6440\n",
      "   macro avg       0.91      0.92      0.91      6440\n",
      "weighted avg       0.94      0.94      0.94      6440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"Class {}\".format(i) for i in range(4)]\n",
    "print(classification_report(y_true, predicted_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "gen = ImageDataGenerator(rotation_range=7, width_shift_range=0.05, shear_range=0.05,\n",
    "                               height_shift_range=0.05, zoom_range=0.1)\n",
    "datagen = gen.flow(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "795/795 [==============================] - 20s 26ms/step - loss: 0.8930 - acc: 0.6361\n",
      "Epoch 2/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.7994 - acc: 0.6873\n",
      "Epoch 3/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.7698 - acc: 0.7019\n",
      "Epoch 4/50\n",
      "795/795 [==============================] - 21s 26ms/step - loss: 1.1720 - acc: 0.5259\n",
      "Epoch 5/50\n",
      "795/795 [==============================] - 20s 26ms/step - loss: 0.9353 - acc: 0.6340\n",
      "Epoch 6/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.8116 - acc: 0.6909\n",
      "Epoch 7/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.7268 - acc: 0.7392\n",
      "Epoch 8/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 1.0258 - acc: 0.6031\n",
      "Epoch 9/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.9176 - acc: 0.6191\n",
      "Epoch 10/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.7116 - acc: 0.7396\n",
      "Epoch 11/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.9809 - acc: 0.5610\n",
      "Epoch 12/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.6542 - acc: 0.7688\n",
      "Epoch 13/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 1.1194 - acc: 0.4852\n",
      "Epoch 14/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 1.0633 - acc: 0.5319\n",
      "Epoch 15/50\n",
      "795/795 [==============================] - 20s 26ms/step - loss: 0.9158 - acc: 0.6545\n",
      "Epoch 16/50\n",
      "795/795 [==============================] - 20s 26ms/step - loss: 0.9511 - acc: 0.6053\n",
      "Epoch 17/50\n",
      "795/795 [==============================] - 20s 26ms/step - loss: 0.8182 - acc: 0.6981\n",
      "Epoch 18/50\n",
      "795/795 [==============================] - 20s 26ms/step - loss: 0.7557 - acc: 0.7200\n",
      "Epoch 19/50\n",
      "795/795 [==============================] - 21s 26ms/step - loss: 0.7520 - acc: 0.7197\n",
      "Epoch 20/50\n",
      "795/795 [==============================] - 21s 26ms/step - loss: 0.7279 - acc: 0.7426\n",
      "Epoch 21/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.7616 - acc: 0.7202\n",
      "Epoch 22/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.7410 - acc: 0.7165\n",
      "Epoch 23/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.7582 - acc: 0.7080\n",
      "Epoch 24/50\n",
      "795/795 [==============================] - 21s 26ms/step - loss: 0.7146 - acc: 0.7444\n",
      "Epoch 25/50\n",
      "795/795 [==============================] - 26s 32ms/step - loss: 0.8384 - acc: 0.6661\n",
      "Epoch 26/50\n",
      "795/795 [==============================] - 25s 32ms/step - loss: 0.7067 - acc: 0.7433\n",
      "Epoch 27/50\n",
      "795/795 [==============================] - 25s 32ms/step - loss: 0.8972 - acc: 0.6626\n",
      "Epoch 28/50\n",
      "795/795 [==============================] - 25s 32ms/step - loss: 0.8469 - acc: 0.6863\n",
      "Epoch 29/50\n",
      "795/795 [==============================] - 25s 32ms/step - loss: 0.7485 - acc: 0.7254\n",
      "Epoch 30/50\n",
      "795/795 [==============================] - 25s 32ms/step - loss: 0.7212 - acc: 0.7457\n",
      "Epoch 31/50\n",
      "795/795 [==============================] - 25s 31ms/step - loss: 0.7069 - acc: 0.7464\n",
      "Epoch 32/50\n",
      "795/795 [==============================] - 25s 32ms/step - loss: 0.6771 - acc: 0.7638\n",
      "Epoch 33/50\n",
      "795/795 [==============================] - 25s 32ms/step - loss: 0.7483 - acc: 0.7051\n",
      "Epoch 34/50\n",
      "795/795 [==============================] - 26s 32ms/step - loss: 0.6524 - acc: 0.7742\n",
      "Epoch 35/50\n",
      "795/795 [==============================] - 25s 32ms/step - loss: 0.7231 - acc: 0.7441\n",
      "Epoch 36/50\n",
      "795/795 [==============================] - 26s 32ms/step - loss: 0.6755 - acc: 0.7677\n",
      "Epoch 37/50\n",
      "795/795 [==============================] - 25s 31ms/step - loss: 0.6906 - acc: 0.7645\n",
      "Epoch 38/50\n",
      "795/795 [==============================] - 22s 28ms/step - loss: 1.0419 - acc: 0.5386\n",
      "Epoch 39/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.7442 - acc: 0.7148\n",
      "Epoch 40/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.7041 - acc: 0.7322\n",
      "Epoch 41/50\n",
      "795/795 [==============================] - 20s 26ms/step - loss: 0.6930 - acc: 0.7559\n",
      "Epoch 42/50\n",
      "795/795 [==============================] - 20s 26ms/step - loss: 0.7539 - acc: 0.7067\n",
      "Epoch 43/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.8830 - acc: 0.6440\n",
      "Epoch 44/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.7246 - acc: 0.7392\n",
      "Epoch 45/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.6884 - acc: 0.7573\n",
      "Epoch 46/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.6514 - acc: 0.7804\n",
      "Epoch 47/50\n",
      "795/795 [==============================] - 20s 26ms/step - loss: 0.7205 - acc: 0.7435\n",
      "Epoch 48/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 1.2187 - acc: 0.4267\n",
      "Epoch 49/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.9138 - acc: 0.6399\n",
      "Epoch 50/50\n",
      "795/795 [==============================] - 20s 25ms/step - loss: 0.8402 - acc: 0.6976\n"
     ]
    }
   ],
   "source": [
    "test1 = cnn1.fit_generator(datagen, epochs = 50, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_results(model, column):\n",
    "    yhat = model.predict(X_test)\n",
    "    _ , ytest = pick_ylabels(column)\n",
    "    accuracy = accuracy_score(ytest, yhat)\n",
    "    print( 'Accuracy Score: ', accuracy )\n",
    "    recall = recall_score(ytest, yhat, average='weighted')\n",
    "    print( 'Recall Score: ', recall)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    wrong_id_list = []\n",
    "    pred_cat_list = []\n",
    "    real_cat_list = []\n",
    "    for row_idx in range(len(ytest)):\n",
    "        if ytest[row_idx]!=yhat[row_idx]:\n",
    "            wrong_id_list.append(row_idx)\n",
    "            pred_cat_list.append(yhat[row_idx])\n",
    "            real_cat_list.append(ytest[row_idx])\n",
    "\n",
    "    arr = np.array([ real_cat_list, pred_cat_list])\n",
    "    arr = arr.transpose()\n",
    "    wrong_df = pd.DataFrame( arr, index= wrong_id_list, columns = ['actual', 'predicted'] )\n",
    "    return (accuracy, recall, wrong_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
